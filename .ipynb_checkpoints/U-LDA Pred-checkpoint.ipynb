{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1ea2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79563576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv file of pre-processed, tokenized articles\n",
    "df = pd.read_csv('250k_full_lda_185.csv')\n",
    "#df['summary_words'] = [eval(item) for item in df['summary_words']]\n",
    "df['article_words'] = [eval(item) for item in df['article_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f6b115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>article_words</th>\n",
       "      <th>lda_topic</th>\n",
       "      <th>other_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>539903</td>\n",
       "      <td>539903</td>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>James Vincent</td>\n",
       "      <td>Misfit’s new tool lets you customize your fitn...</td>\n",
       "      <td>Misfit has launched a new tool that lets users...</td>\n",
       "      <td>https://www.theverge.com/circuitbreaker/2017/6...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>[misfit, launch, new, tool, let, user, persona...</td>\n",
       "      <td>28</td>\n",
       "      <td>[128]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1                 date  year  month  \\\n",
       "0           0        539903          539903  2017-06-01 00:00:00  2017    6.0   \n",
       "\n",
       "   day         author                                              title  \\\n",
       "0    1  James Vincent  Misfit’s new tool lets you customize your fitn...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Misfit has launched a new tool that lets users...   \n",
       "\n",
       "                                                 url section publication  \\\n",
       "0  https://www.theverge.com/circuitbreaker/2017/6...    Tech   The Verge   \n",
       "\n",
       "                                       article_words  lda_topic other_topics  \n",
       "0  [misfit, launch, new, tool, let, user, persona...         28        [128]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b76016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c7e182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>article_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1527764</td>\n",
       "      <td>1527764</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RPT-COLUMN-China's metals trade fragments into...</td>\n",
       "      <td>(Repeats Aug. 25 column without change. The op...</td>\n",
       "      <td>https://www.reuters.com/article/china-metals-a...</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>[repeats, aug, column, without, change, opinio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168893</td>\n",
       "      <td>168893</td>\n",
       "      <td>2016-04-08 15:50:02</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>David Roberts</td>\n",
       "      <td>New music for you: Edna Vazquez plays her song...</td>\n",
       "      <td>Once a month (or so), I'm bringing you an epis...</td>\n",
       "      <td>https://www.vox.com/culture/2016/4/8/11391828/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox</td>\n",
       "      <td>[month, bring, episode, pickathon, pumphouse, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001409</td>\n",
       "      <td>2001409</td>\n",
       "      <td>2016-09-09 00:35:26</td>\n",
       "      <td>2016</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>One Wild Pitch Leads to Two Runs as Indians Be...</td>\n",
       "      <td>Sports Briefing The Indians scored two runs on...</td>\n",
       "      <td>http://www.nytimes.com/2016/09/09/sports/baseb...</td>\n",
       "      <td>sports</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[sports, briefing, indians, score, two, run, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358141</td>\n",
       "      <td>358141</td>\n",
       "      <td>2018-03-01 09:09:08</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ron Miller</td>\n",
       "      <td>Dropbox to add native G Suite integration in n...</td>\n",
       "      <td>It’s been an eventful week for Dropbox coming ...</td>\n",
       "      <td>https://techcrunch.com/2018/03/01/dropbox-to-a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>[eventful, week, dropbox, come, announcement, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307943</td>\n",
       "      <td>307943</td>\n",
       "      <td>2017-12-14 00:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Christopher Matthews</td>\n",
       "      <td>Walmart will start giving salary advances to 1...</td>\n",
       "      <td>Walmart announced Wednesday that it will start...</td>\n",
       "      <td>https://www.axios.com/walm-1513267182-9615b668...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Axios</td>\n",
       "      <td>[walmart, announce, wednesday, start, allow, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 0.1                 date  year  month day  \\\n",
       "0     1527764      1527764           2017-08-28  2017    8.0  28   \n",
       "1      168893       168893  2016-04-08 15:50:02  2016    4.0   8   \n",
       "2     2001409      2001409  2016-09-09 00:35:26  2016    9.0   9   \n",
       "3      358141       358141  2018-03-01 09:09:08  2018    3.0   1   \n",
       "4      307943       307943  2017-12-14 00:00:00  2017   12.0  14   \n",
       "\n",
       "                 author                                              title  \\\n",
       "0                   NaN  RPT-COLUMN-China's metals trade fragments into...   \n",
       "1         David Roberts  New music for you: Edna Vazquez plays her song...   \n",
       "2  The Associated Press  One Wild Pitch Leads to Two Runs as Indians Be...   \n",
       "3            Ron Miller  Dropbox to add native G Suite integration in n...   \n",
       "4  Christopher Matthews  Walmart will start giving salary advances to 1...   \n",
       "\n",
       "                                             article  \\\n",
       "0  (Repeats Aug. 25 column without change. The op...   \n",
       "1  Once a month (or so), I'm bringing you an epis...   \n",
       "2  Sports Briefing The Indians scored two runs on...   \n",
       "3  It’s been an eventful week for Dropbox coming ...   \n",
       "4  Walmart announced Wednesday that it will start...   \n",
       "\n",
       "                                                 url section  \\\n",
       "0  https://www.reuters.com/article/china-metals-a...   Intel   \n",
       "1  https://www.vox.com/culture/2016/4/8/11391828/...     NaN   \n",
       "2  http://www.nytimes.com/2016/09/09/sports/baseb...  sports   \n",
       "3  https://techcrunch.com/2018/03/01/dropbox-to-a...     NaN   \n",
       "4  https://www.axios.com/walm-1513267182-9615b668...     NaN   \n",
       "\n",
       "          publication                                      article_words  \n",
       "0             Reuters  [repeats, aug, column, without, change, opinio...  \n",
       "1                 Vox  [month, bring, episode, pickathon, pumphouse, ...  \n",
       "2  The New York Times  [sports, briefing, indians, score, two, run, d...  \n",
       "3          TechCrunch  [eventful, week, dropbox, come, announcement, ...  \n",
       "4               Axios  [walmart, announce, wednesday, start, allow, w...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_pickle('p25million.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40065e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_compute(df,num_topics,passes=20):\n",
    "    start = time.time()\n",
    "    id2word = corpora.Dictionary(df['article_words'])\n",
    "    \n",
    "    articles = df['article_words']\n",
    "    \n",
    "    \n",
    "    corpus = [id2word.doc2bow(article) for article in articles]\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=num_topics,\n",
    "                                           passes=passes,random_state=4)\n",
    "    \n",
    "#    print(lda_model.print_topics())\n",
    "    print('-' * 10)\n",
    "    doc_lda = lda_model[corpus]\n",
    "    \n",
    "    lda_lists = [lda_model[article] for article in corpus]\n",
    "    print(f'lda_lists[-1]: {lda_lists[-1]}')\n",
    "    lda_coeffs = [[pair[1] for pair in article] for article in lda_lists]\n",
    "    print(f'lda_coeffs[-1]: {lda_coeffs[-1]}')\n",
    "    lda_inds = [[pair[0] for pair in article] for article in lda_lists]\n",
    "    print(f'lda_inds[-1]: {lda_inds[-1]}')\n",
    "    \n",
    "    rogues = [i for i in range(len(lda_inds)) if len(lda_inds[i])==0]\n",
    "    \n",
    "    lda_lists = [lda_lists[i] for i in range(len(lda_lists)) if i not in rogues]\n",
    "    lda_coeffs = [lda_coeffs[i] for i in range(len(lda_coeffs)) if i not in rogues]\n",
    "    lda_inds = [lda_inds[i] for i in range(len(lda_inds)) if i not in rogues]\n",
    "    \n",
    "    df.drop(rogues,inplace=True,axis=0)\n",
    "    \n",
    "    \n",
    "#    df_rogue = df.loc[rogues,:]\n",
    "#    return df_rogue \n",
    "    \n",
    "###    lda_argmax = [np.argmax(tup) if len(tup) < 0 else None for tup in lda_coeffs]\n",
    "#    lda_args = [np.arsgort(tup)[::-1] for tup in lda_coeffs]\n",
    "    \n",
    "    lda_argmax = [np.argmax(tup) for tup in lda_coeffs]\n",
    "    \n",
    "    \n",
    "    print(f'lda_argmax[-1]: {lda_argmax[-1]}')\n",
    "#    lda_inds = [pair[0] for pair in lda_tuples]\n",
    "#    print(lda_inds[-1])\n",
    "#    print('-' * 10)\n",
    "#    print(lda_argmax[-1])\n",
    "#    print('-' * 10)\n",
    "###    lda_topics = [lda_inds[i][lda_argmax[i]] if lda_argmax[i] else None for i in range(len(lda_lists))]\n",
    "    lda_topics = [lda_inds[i][lda_argmax[i]] for i in range(len(lda_lists))]\n",
    "    \n",
    "    sec_threshold = 0.5\n",
    "    \n",
    "#    pdb.set_trace()\n",
    "    \n",
    "#    secondary_topics = [[coeff for coeff in lda_coeffs[i] if 1 > (coeff / lda_coeffs[i][lda_argmax[i]]) >= sec_threshold] for i in range(len(lda_lists))]\n",
    "        \n",
    "    \n",
    "#    secondary_topics = [[coeff for coeff in lda_coeffs[i] if 1 > (coeff / lda_coeffs[i][lda_argmax[i]]) >= sec_threshold] for i in range(len(lda_lists))]\n",
    "    \n",
    "    \n",
    "#    secondary topics = [[lda_inds[j] for j in range(len(lda_inds[i])) if lda_coeffs[j] / lda_coeffs[i][lda_argmax[i]] >= sec_threshold] for i in range(len(lda_lists))]      \n",
    "    \n",
    "    secondary_topics = [[pair[0] for pair in lda_lists[i] if 1 > pair[1] / lda_coeffs[i][lda_argmax[i]] >= sec_threshold] for i in range(len(lda_lists))]\n",
    "    \n",
    "    df['lda_topic'] = lda_topics\n",
    "    \n",
    "    df['other_topics'] = secondary_topics   \n",
    "    \n",
    "    interval = round((time.time() - start)/60,2)\n",
    "    print(f'That  took {interval} mins.')\n",
    "    return df, lda_model, corpus#, doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33baf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dft = df[:20000]\n",
    "dft = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7eebaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95feaac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d995d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fergu\\Anaconda3\\lib\\multiprocessing\\queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\fergu\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "MemoryError\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a76afe3f3373>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#19:38\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m190\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-4398cc959ee3>\u001b[0m in \u001b[0;36mlda_compute\u001b[1;34m(df, num_topics, passes)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=num_topics,\n\u001b[0m\u001b[0;32m     11\u001b[0m                                            passes=passes,random_state=4)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"auto-tuning alpha not implemented in multicore LDA; use plain LdaModel.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         super(LdaMulticore, self).__init__(\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[1;34m(force)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \"\"\"\n\u001b[0;32m    267\u001b[0m             \u001b[0mmerged_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mempty\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    853\u001b[0m                     \u001b[1;31m# start an overlapped read of length zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m                         \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileno\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                         \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwinerror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#16:30\n",
    "dft, lda_model, corpus = lda_compute(dft,190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3e2295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and df to desired filenames\n",
    "dft.to_csv('test_df.csv')\n",
    "lda_model.save('test_ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107a55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93d96fe4",
   "metadata": {},
   "source": [
    "## Word2vec Alternative to LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8296474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w2v(col1,col2,max_epochs,vec_size,window,min_count,resume=False):\n",
    "    now  = datetime.now()\n",
    "    start_time_str = now.strftime('%H:%M:%S')\n",
    "    print('started at',start_time_str)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    word_list = [[words for words in line] for line in col1]\n",
    "    unpacked_word_list = [word for line in word_list for word in line]\n",
    "    \n",
    "    filename = 'model.sav'\n",
    "    \n",
    "    if resume == False:\n",
    "        model = Word2Vec(col1,vector_size=vec_size,window=window,min_count=min_count,workers=-1)\n",
    "        model.train(col1,total_examples=len(col1),epochs=max_epochs)\n",
    "        print('word2vec model has been trained.')\n",
    "        pickle.dump(model,open(filename,'wb'))\n",
    "    else:\n",
    "        file = open(filename,'rb')\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    averaged_vectors = [] if resume == False else pd.read_csv('intermediate_results.csv',index_col=0)\n",
    "    \n",
    "    if resume == True:\n",
    "        averaged_vectors = [list(averaged_vectors.loc[val,:]) for val in averaged_vectors.index]\n",
    "    \n",
    "    j = start_index= len(averaged_vectors)\n",
    "    if j != 0:\n",
    "        print(f'{round(j/len(col1),3) * 100}% complete.')\n",
    "    start_checkpoint_time = time.time()\n",
    "    \n",
    "    progress_indicators = []\n",
    "    \n",
    "    \n",
    "    for row in range(j,len(col1)):\n",
    "        list_of_words = col1[row]\n",
    "        list_of_vectors = [model.wv[word] for word in list_of_words if unpacked_word_list.count(word) >= min_count]\n",
    "        avg_vector = np.mean(list_of_vectors,axis=0) if list_of_vectors else np.zeros(vec_size)\n",
    "        averaged_vectors.append(avg_vector)\n",
    "#        print(avg_vector)\n",
    "        \n",
    "        j += 1\n",
    "#        print(f'{j} done')\n",
    "        if j % 1000 == 0:\n",
    "            intermediate_results = pd.DataFrame(averaged_vectors)\n",
    "            intermediate_results.to_csv('intermediate_results.csv')\n",
    "            \n",
    "        percent_complete = round(j/len(col1) * 100)\n",
    "        if (percent_complete % 1 == 0) & (percent_complete not in progress_indicators):\n",
    "            cum_time_raw = (time.time() - start_time)/60\n",
    "            cum_time = round((time.time() - start_time)/60,1)\n",
    "            print('{}% complete. Time elapsed: {} min'.format(percent_complete,cum_time))\n",
    "            progress_indicators.append(percent_complete)\n",
    "            \n",
    "            remaining_time = cum_time_raw * (len(col1) - j) / (j - start_index)\n",
    "            \n",
    "            est_finish_time = datetime.now() + timedelta(minutes=remaining_time)\n",
    "            est_finish_time = est_finish_time.strftime('%D:%H:%M:%S')\n",
    "            print(f'Estimated finish time: {est_finish_time}')\n",
    "            print('-' * 15)\n",
    "            \n",
    "    df_of_vectors = pd.DataFrame(averaged_vectors)\n",
    "        \n",
    "    intermediate_table = pd.concat([col2,df_of_vectors],axis=1)\n",
    "        \n",
    "    rows_to_remove = [ind for ind in intermediate_table.index if intermediate_table.iloc[ind,2] == 0]\n",
    "        \n",
    "    removed_comments = pd.DataFrame(col2[rows_to_remove])\n",
    "    final_table = intermediate_table.drop(rows_to_remove,axis=0)\n",
    "        \n",
    "    interval = round((time.time() - start_time)/60,1)\n",
    "    print('-' * 25)\n",
    "    print(f'Finished. That took {interval} min.')\n",
    "    now = datetime.now()\n",
    "    end_time_str = now.strftime('%H:%M:%S')\n",
    "    print(f'Actual finish time: {end_time_str}')\n",
    "        \n",
    "    return final_table, removed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec56477",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table, removed_comments = create_w2v(tab['article_words'],tab['tweet'],5,100,5,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
